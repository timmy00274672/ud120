{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pwd is `ud120-projects\\datasets_questions` now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 13: Size Of The Enron Dataset\n",
    "\n",
    "The aggregated Enron email + financial dataset is stored in a dictionary, where each key in the dictionary is a person’s name and the value is a dictionary containing all the features of that person.\n",
    "The email + finance (E+F) data dictionary is stored as a pickle file, which is a handy way to store and load python objects directly. Use **datasets_questions/explore_enron_data.py** to load the dataset.\n",
    "\n",
    "How many data points (people) are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "enron_data = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\"))\n",
    "print len(enron_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 14 : Features In The Enron Dataset\n",
    "\n",
    "For each person, how many features are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "for person, attr in enron_data.items():\n",
    "    print len(attr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 15: Finding POIs In The Enron Data\n",
    "\n",
    "The “poi” feature records whether the person is a person of interest, according to our definition. How many POIs are there in the E+F dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "poi_in_dataset = [x for x,y in enron_data.items() if y['poi']]\n",
    "print len(poi_in_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 16: How Many POIs Exist?\n",
    "\n",
    "We compiled a list of all POI names (in ../final_project/poi_names.txt) and associated email addresses (in ../final_project/poi_email_addresses.py).\n",
    "\n",
    "How many POI’s were there total? (Use the names file, not the email addresses, since many folks have more than one address and a few didn’t work for Enron, so we don’t have their emails.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ea8f33746a8d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-ea8f33746a8d>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %load ../final_project/poi_names.txt\n",
    "http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm\n",
    "\n",
    "(y) Lay, Kenneth\n",
    "(y) Skilling, Jeffrey\n",
    "(n) Howard, Kevin\n",
    "(n) Krautz, Michael\n",
    "(n) Yeager, Scott\n",
    "(n) Hirko, Joseph\n",
    "(n) Shelby, Rex\n",
    "(n) Bermingham, David\n",
    "(n) Darby, Giles\n",
    "(n) Mulgrew, Gary\n",
    "(n) Bayley, Daniel\n",
    "(n) Brown, James\n",
    "(n) Furst, Robert\n",
    "(n) Fuhs, William\n",
    "(n) Causey, Richard\n",
    "(n) Calger, Christopher\n",
    "(n) DeSpain, Timothy\n",
    "(n) Hannon, Kevin\n",
    "(n) Koenig, Mark\n",
    "(y) Forney, John\n",
    "(n) Rice, Kenneth\n",
    "(n) Rieker, Paula\n",
    "(n) Fastow, Lea\n",
    "(n) Fastow, Andrew\n",
    "(y) Delainey, David\n",
    "(n) Glisan, Ben\n",
    "(n) Richter, Jeffrey\n",
    "(n) Lawyer, Larry\n",
    "(n) Belden, Timothy\n",
    "(n) Kopper, Michael\n",
    "(n) Duncan, David\n",
    "(n) Bowen, Raymond\n",
    "(n) Colwell, Wesley\n",
    "(n) Boyle, Dan\n",
    "(n) Loehr, Christopher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "# remove the first two lines\n",
    "\n",
    "file = open('../final_project/poi_names.txt')\n",
    "lines = file.readlines()\n",
    "print len(lines[2:])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 17: Problems With Incomplete Data\n",
    "\n",
    "As you can see, we have many of the POIs in our E+F dataset, but not all of them. Why is that a potential problem?\n",
    "\n",
    "We will return to this later to explain how a POI could end up not being in the Enron E+F dataset, so you fully understand the issue before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because not all POIs were employed by enron and the data was just subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things you could say here, but our main thought is about having enough data to really learn the patterns. In general, more data is always better--only having 18 data points doesn't give you that many examples to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 18: Query The Dataset 1\n",
    "\n",
    "Like any dict of dicts, individual people/features can be accessed like so:\n",
    "\n",
    "enron_data[\"LASTNAME FIRSTNAME\"][\"feature_name\"]\n",
    "or, sometimes \n",
    "enron_data[\"LASTNAME FIRSTNAME MIDDLEINITIAL\"][\"feature_name\"]\n",
    "\n",
    "What is the total value of the stock belonging to James Prentice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_data['Prentice James'.upper()]['total_stock_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 19: Query The Dataset 2\n",
    "\n",
    "Like any dict of dicts, individual people/features can be accessed like so:\n",
    "\n",
    "enron_data[\"LASTNAME FIRSTNAME\"][\"feature_name\"]\n",
    "\n",
    "How many email messages do we have from Wesley Colwell to persons of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_data['Colwell Wesley'.upper()]['from_this_person_to_poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 20: Query The Dataset 3\n",
    "\n",
    "Like any dict of dicts, individual people/features can be accessed like so:\n",
    "\n",
    "enron_data[\"LASTNAME FIRSTNAME\"][\"feature_name\"]\n",
    "\n",
    "or\n",
    "\n",
    "enron_data[\"LASTNAME FIRSTNAME MIDDLEINITIAL\"][\"feature_name\"]\n",
    "\n",
    "What’s the value of stock options exercised by Jeffrey K Skilling?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19250000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_data['Skilling Jeffrey K'.upper()]['exercised_stock_options']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 21: Research The Enron Fraud\n",
    "\n",
    "In the coming lessons, we’ll talk about how the best features are often motivated by our human understanding of the problem at hand. In this case, that means knowing a little about the story of the Enron fraud.\n",
    "\n",
    "If you have an hour and a half to spare, “Enron: The Smartest Guys in the Room” is a documentary that gives an amazing overview of the story. Alternatively, there are plenty of archival newspaper stories that chronicle the rise and fall of Enron.\n",
    "\n",
    "Which of these schemes was Enron not involved in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [o] selling assets to shell companies at the end of each month, and buying them back at the beginning of the next month to hide accounting losses\n",
    "- [o] causing electrical grid failures in California\n",
    "- [x] illegally obtained a government report that enabled them to corner the market on frozen concentrated orange juice futures\n",
    "- [x] conspiring to give a Saudi prince expedited American citizenship\n",
    "- [o] a plan in collaboration with Blockbuster movies to stream movies over the internet\n",
    "\n",
    "That's right, only 2 of these schemes are made up.  By the way--the orange juice futures scheme was the plot of \"Trading Places,\" and the citizenship fraud was \"American Hustle.\"  We crack ourselves up sometimes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 22: Enron CEO\n",
    "\n",
    "Who was the CEO of Enron during most of the time that fraud was being perpetrated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeffrey Skilling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 23: Enron Chairman\n",
    "\n",
    "Who was chairman of the Enron board of directors?\n",
    "Who was chairman of the Enron board of directors during much of the time that fraud was ongoing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kenneth Lay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 24: Enron CFO\n",
    "Who was CFO (chief financial officer) of Enron during most of the time that fraud was going on?\n",
    "Who was CFO during much of the time that fraud was ongoing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Fastow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 25: Follow The Money\n",
    "Of these three individuals (Lay, Skilling and Fastow), who took home the most money (largest value of “total_payments” feature)?\n",
    "\n",
    "How much money did that person get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAY KENNETH L took 103559793\n",
      "SKILLING JEFFREY K took 8682716\n",
      "FASTOW ANDREW S took 2424083\n"
     ]
    }
   ],
   "source": [
    "list = ['LAY KENNETH L', 'SKILLING JEFFREY K', 'FASTOW ANDREW S']\n",
    "for name in list:\n",
    "    print '{} took {}'.format(name, enron_data[name]['total_payments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 26: Unfilled Features\n",
    "For nearly every person in the dataset, not every feature has a value. How is it denoted when a feature doesn’t have a well-defined value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NaN'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_data['BADUM JAMES P']['bonus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN aka Not a Number is the correct response!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 27: Dealing With Unfilled Features\n",
    "How many folks in this dataset have a quantified salary? What about a known email address?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 mails are known\n",
      "95 salary are known\n"
     ]
    }
   ],
   "source": [
    "mail_known = 0\n",
    "salary_known = 0\n",
    "for name, attrs in enron_data.iteritems():\n",
    "    if(attrs['email_address'] != 'NaN'):\n",
    "        mail_known += 1\n",
    "    if(attrs['salary'] != 'NaN'):\n",
    "        salary_known += 1\n",
    "print '{} mails are known'.format(mail_known)\n",
    "print '{} salary are known'.format(salary_known)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pandas` to make the code much cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has salary data: 95\n",
      "Has email: 111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(enron_data)\n",
    "print 'Has salary data: {0}'.format(sum(df.loc['salary',:] != 'NaN'))\n",
    "print 'Has email: {0}'.format(sum(df.loc['email_address',:] != 'NaN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict-to-array conversion\n",
    "\n",
    "A python dictionary **can’t** be read directly into an sklearn classification or regression algorithm; instead, it needs a **numpy array** or a list of lists (each element of the list (itself a list) is a data point, and the elements of the smaller list are the features of that point).\n",
    "\n",
    "We’ve written some helper functions (featureFormat() and targetFeatureSplit() in tools/feature_format.py) that can take a list of feature names and the data dictionary, and return a numpy array.\n",
    "\n",
    "In the case when a feature does not have a value for a particular person, this function will also replace the feature value with 0 (zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../tools/feature_format.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 29: Missing POIs 1 (Optional)\n",
    "As you saw a little while ago, not every POI has an entry in the dataset (e.g. Michael Krautz). That’s because the dataset was created using the financial data you can find in final_project/enron61702insiderpay.pdf, which is missing some POI’s (those absences propagated through to the final dataset). On the other hand, for many of these “missing” POI’s, we do have emails.\n",
    "\n",
    "While it would be straightforward to add these POI’s and their email information to the E+F dataset, and just put “NaN” for their financial information, this could introduce a subtle problem. You will walk through that here.\n",
    "\n",
    "How many people in the E+F dataset (as it currently exists) have “NaN” for their total payments? What percentage of people in the dataset as a whole is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_payments == 'NaN': 21 people = 14.38%\n"
     ]
    }
   ],
   "source": [
    "subset = {p: a for p, a in enron_data.items() if a['total_payments'] == 'NaN'}\n",
    "print 'total_payments == \\'NaN\\': {0} people = {1:.2f}%'.format(len(subset), 100.*len(subset)/len(enron_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_payments == 'NaN': 21 people = 14.38%\n"
     ]
    }
   ],
   "source": [
    "isnan = sum(df.loc['total_payments',:]=='NaN')\n",
    "_,cols = df.shape\n",
    "print 'total_payments == \\'NaN\\': {0} people = {1:.2f}%'.format(isnan, 100.*isnan/cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 30: Missing POIs 2 (Optional)\n",
    "How many POIs in the E+F dataset have “NaN” for their total payments? What percentage of POI’s as a whole is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI total_payments == 'NaN': 0 people = 0.00%\n"
     ]
    }
   ],
   "source": [
    "subset = {p: a for p, a in enron_data.items() \n",
    "          if a['total_payments'] == 'NaN' and a['poi']}\n",
    "print 'POI total_payments == \\'NaN\\': {0} people = {1:.2f}%'.format(len(subset), 100.*len(subset)/len(enron_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "subset = {p: a for p, a in enron_data.items() if a['poi']}\n",
    "print(len(subset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
